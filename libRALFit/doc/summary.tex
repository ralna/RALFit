% The beginning summary section
{\tt \fullpackagename} computes a solution $\vx$ to the non-linear least-squares problem
\begin{equation}
\min_\vx \  F(\vx) := \frac{1}{2}\| \vr(\vx) \|_{\vW}^2 + \frac{\sigma}{p}\| \vx\|_2^p,
\label{eq:nlls_problem}
\end{equation}
where $\vW\in\mathbb{R}^{m\times m}$ is a diagonal, non-negative, weighting matrix, and $\vr(\vx) =(\comp[1]{r}(\vx), \comp[2]{r}(\vx),...,\comp[m]{r}(\vx))^T$ is a non-linear function.

A typical use may be to fit a function $f(\vx)$ to the data $y_i, \ t_i$, weighted by the uncertainty of the data, $\sigma_i$, so that
$$r_i(\vx) := y_i - f(\vx;t_i),$$
and $\vW$ is the diagonal matrix such that $\vW_{ii} = (1/\sqrt{\sigma_i}).$  For this reason 
we refer to the function $\vr$ as the \emph{residual} function.
% the fit of the data $y$ to some non-linear function ${\bm f} : \mathbb{R}^n \rightarrow \mathbb{R}^m$
% ($m>n$).
% The $n$ variables that are fitted are $\vx=(x_1,x_2,...,x_n)^T$.
% \textcolor{blue}{Some confusion: the $y_i$ don't appear again.}

The algorithm is iterative.
At each point, $\iter{\vx}$, the algorithm builds a model of the function at the next step, $F({\iter{\vx}+\iter{\vs}})$, which we refer to as $m_k(\cdot)$.  We allow either a Gauss-Newton model, a (quasi-)Newton model, or a Newton-tensor model; see Section \ref{sec:model_description} for more details about these models.  

Once the model has been formed we find a candidate for the next step by either solving a trust-region sub-problem of the form
\begin{equation}
\iter{\vs} = \arg \min_{\vs} \ \iter{m} (\vs) \quad \mathrm{s.t.} \quad  \|\vs\|_B \leq \Delta_k,\label{eq:tr_subproblem}
\end{equation}
or by solving the regularized problem 
\begin{equation}
\iter{\vs} = \arg \min_{\vs} \ \iter{m} (\vs)  + \frac{1}{\Delta_k}\cdot \frac{1}{p} \|\vs\|_B^p,\label{eq:reg_subproblem}
\end{equation}
where $\Delta_k$ is a parameter of the algorithm (the trust region radius or the inverse of the regularization parameter respectively), $p$ is a given integer, and $B$ is a symmetric positive definite weighting matrix that is calculated by the algorithm.
The quantity
\[\rho = \frac{F(\iter{\vx}) - F(\iter{\vx} + \iter{\vs})}{\iter{m}(\iter{\vx}) - \iter{m}(\iter{\vx} + \iter{\vs})}\]
is then calculated.
If this is sufficiently large we accept the step, and $\iter[k+1]{\vx}$ is set to $\iter{\vx} + \iter{\vs}$; if not, the parameter $\Delta_k$
is reduced and  the resulting new trust-region sub-problem is solved.  If the step is very successful -- in that $\rho$ is close to one --
$\Delta_k$ is increased.

This process continues until either the residual, $\|\vr(\iter{\vx})\|_\vW$, or a measure of the gradient,
$\|{\iter{\vJ}}^T\vW\vr(\iter{\vx})\|_2 / \|\vr(\iter{\vx})\|_\vW$, is sufficiently small.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "nlls_fortran"
%%% End: 
